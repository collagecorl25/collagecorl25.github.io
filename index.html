<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description" content="COLLAGE enables task-adaptive multi-feature data retrieval for few-shot imitation learning." />
  <title>COLLAGE: Adaptive Fusion-based Retrieval for Augmented Policy Learning</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />
  <link rel="stylesheet" href="./static/css/bulma.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@glidejs/glide@3.6.0/dist/css/glide.core.min.css" />
  <link rel="stylesheet" href="./static/css/index.css" />
</head>
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">COLLAGE: Adaptive Fusion-based Retrieval for Augmented Policy Learning</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">Anonymous Authors</span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Abstract -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              In this work, we study the problem of data retrieval for few-shot imitation learning: selecting data from a large dataset to train a performant policy for a specific task, given only a few target demonstrations. Prior methods retrieve data using a single-feature distance heuristic, assuming that the best demonstrations are those that most closely resemble the target examples in visual, semantic, or motion space. However, this approach captures only a subset of the relevant information and is prone to introducing detrimental demonstrations, such as those from unrelated tasks with similar scene layouts or tasks with similar motion but divergent goals.
            </p>
            <p>
              We present <strong>COLLAGE</strong>, a method for <strong>COLL</strong>ective data <strong>AG</strong>gr<strong>E</strong>gation in few-shot imitation learning that uses an adaptive late fusion mechanism to guide the selection of relevant demonstrations based on a task-specific combination of multiple cues. COLLAGE assigns weights to subsets of the dataset pre-selected using single-feature retrieval (e.g., appearance, shape, or language similarity), based on how well a policy trained on each subset predicts the few target demonstrations. These weights are then used during training to importance-sample data across the retrieved subsets. This strategy is general, feature-agnostic, and flexible, enabling COLLAGE to leverage complementary information and outperform both single-modality and multitask baselines. In extensive experiments, COLLAGE improves average performance by 5.1% in simulation (LIBERO-10) and 16.6% in real-world tasks from the DROID dataset.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Method Overview -->
  <section class="section">
    <div class="container is-max-desktop has-text-centered">
      <h2 class="title is-3">Method Overview</h2>
      <figure class="image">
  <video autoplay loop muted playsinline style="max-width: 100%; border-radius: 8px;">
    <source src="./static/images/method_overview.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <figcaption class="method-caption">
  <strong>Overview of COLLAGE.</strong>
  <strong>Left:</strong> Given a set of target demonstrations, each modality (e.g., visual, motion, shape, or language) retrieves a set of similar (sub-)trajectories from a prior dataset of diverse demonstrations. <strong>Center:</strong> We use the retrieved (sub-)trajectories for each modality to train a reference policy. For each reference policy, we compute the log-likelihood of the target trajectories — that is, how well the policy predicts the target actions at each target state. These log-likelihoods are used to assign importance weights to the modalities. 
   <strong>Right:</strong> We train the final policy using all retrieved data, sampling more frequently from modalities with higher weights.
  </figcaption>
</figure>
    </div>
  </section>

  <!-- Rollout Carousel -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Policy Rollouts with COLLAGE</h2>
      <div class="glide" style="position: relative;">
        <div class="glide__track" data-glide-el="track">
          <ul class="glide__slides">
            <li class="glide__slide"><div class="video-block"><div class="video-card"><video autoplay muted loop playsinline controls><source src="./static/videos/demo_pen_in_cup.mp4" type="video/mp4" /></video></div><p class="video-caption">Pen-Cup</p></div></li>
            <li class="glide__slide"><div class="video-block"><div class="video-card"><video autoplay muted loop playsinline controls><source src="./static/videos/demo_chips_real.mp4" type="video/mp4" /></video></div><p class="video-caption">Chips-Box</p></div></li>
            <li class="glide__slide"><div class="video-block"><div class="video-card"><video autoplay muted loop playsinline controls><source src="./static/videos/demo_umbrella.mp4" type="video/mp4" /></video></div><p class="video-caption">Pick-Umbrella</p></div></li>
            <li class="glide__slide"><div class="video-block"><div class="video-card"><video autoplay muted loop playsinline controls><source src="./static/videos/demo_spatula.mp4" type="video/mp4" /></video></div><p class="video-caption">Stir-Spatula</p></div></li>
            <li class="glide__slide"><div class="video-block"><div class="video-card"><video autoplay muted loop playsinline controls><source src="./static/videos/demo_lego.mp4" type="video/mp4" /></video></div><p class="video-caption">Stack-Lego</p></div></li>
            <li class="glide__slide"><div class="video-block"><div class="video-card"><video autoplay muted loop playsinline controls><source src="./static/videos/demo_scrub.mp4" type="video/mp4" /></video></div><p class="video-caption">Scrub-Plate</p></div></li>
          </ul>
        </div>
        <div class="glide__arrows" data-glide-el="controls">
          <button class="glide__arrow glide__arrow--left" data-glide-dir="<">‹</button>
          <button class="glide__arrow glide__arrow--right" data-glide-dir=">">›</button>
        </div>
        <div class="glide__bullets" data-glide-el="controls[nav]">
          <button class="glide__bullet" data-glide-dir="=0"></button>
          <button class="glide__bullet" data-glide-dir="=1"></button>
          <button class="glide__bullet" data-glide-dir="=2"></button>
          <button class="glide__bullet" data-glide-dir="=3"></button>
          <button class="glide__bullet" data-glide-dir="=4"></button>
          <button class="glide__bullet" data-glide-dir="=5"></button>
        </div>
      </div>
    </div>
  </section>

  <!-- Experiments -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full">
          <h2 class="title is-3">Experiments</h2>
          <div class="content">
            <h3 class="title is-4">Simulated Experiments</h3>
            <figure class="image">
              <img src="./static/images/sim_results_1.png" alt="Simulated experiment results" />
              <img src="./static/images/sim_results_2.png" alt="Simulated experiment results" />
              <figcaption>
                We evaluate COLLAGE on all tasks from the LIBERO-10 benchmark using 5 target demonstrations per task and 4500 demonstrations from LIBERO-90 as the prior dataset. COLLAGE achieves a 10.75% relative performance improvement over the prior state-of-the-art retrieval method STRAP (ICLR ’25). It also outperforms other single-modality retrievals, including POINTNET (Shape) by 14.25% and LANG (Language) by 25.31%.
              </figcaption>
            </figure>

            <h3 class="title is-4" style="margin-top: 1.5em;">Real-World Experiments</h3>
            <figure class="image">
              <img src="./static/images/real_world_results.png" alt="Real-world experiment results" />
              <figcaption>
                Real-world evaluation on six manipulation tasks using the DROID dataset. For each task, we use only 5 target demonstrations and retrieve from a pool of 30k successful episodes. COLLAGE achieves an average success rate of 6.83/15, representing a 58% improvement over STRAP (4.33/15) and a 64% improvement over LANG (4.16/15). Policies trained solely on the 5 in-domain demonstrations (no retrieval) achieve only 1.00/15 success on average. In contrast, COLLAGE effectively leverages relevant demonstrations from DROID to significantly boost policy performance.
              </figcaption>
            </figure>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- Weights Predicted by COLLAGE -->
<<section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Importance Weights Predicted by COLLAGE</h2>
      <div class="has-text-centered">
        <figure class="image" style="max-width: 950px; width: 100%; margin: 0 auto;">
          <img src="./static/images/modality_weights_pie_chart.png" alt="Modality Weights Pie Chart" style="width: 100%; height: auto; border-radius: 8px;">
          <figcaption style="margin-top: 0.75rem; font-size: 1rem; color: #444;">
            <figcaption style="margin-top: 0.75rem;">
                Importance weights assigned by <strong>COLLAGE</strong> to different modalities used in our framework (Visual, Motion, Shape, Language).              
          </figcaption>
        </figure>
      </div>
    </div>
  </section>
  <!-- Footer -->
  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <p>
          This website is based on the <a href="https://nerfies.github.io/">Nerfies</a> website template,
          licensed under a <a href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
        </p>
      </div>
    </div>
  </footer>

  <!-- Glide JS -->
  <script src="https://cdn.jsdelivr.net/npm/@glidejs/glide@3.6.0/dist/glide.min.js"></script>
  <script>
    window.addEventListener('load', function () {
      new Glide('.glide', {
        type: 'carousel',
        rewind: false,
        perView: 3,
        focusAt: 'center',
        gap: 24,
        breakpoints: {
          1024: { perView: 2 },
          768: { perView: 1 }
        }
      }).mount();
    });
  </script>
</body>
</html>
